{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5464849,"sourceType":"datasetVersion","datasetId":3156741}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-24T14:54:07.184251Z","iopub.execute_input":"2023-11-24T14:54:07.185101Z","iopub.status.idle":"2023-11-24T14:54:07.534865Z","shell.execute_reply.started":"2023-11-24T14:54:07.185062Z","shell.execute_reply":"2023-11-24T14:54:07.533912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport math\nfrom torch import nn\nimport torch.nn.functional as Func","metadata":{"execution":{"iopub.status.busy":"2023-12-09T16:56:18.190065Z","iopub.execute_input":"2023-12-09T16:56:18.190874Z","iopub.status.idle":"2023-12-09T16:56:21.001666Z","shell.execute_reply.started":"2023-12-09T16:56:18.190830Z","shell.execute_reply":"2023-12-09T16:56:21.000828Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def get_device():\n    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","metadata":{"execution":{"iopub.status.busy":"2023-12-09T16:56:33.334013Z","iopub.execute_input":"2023-12-09T16:56:33.334503Z","iopub.status.idle":"2023-12-09T16:56:33.339267Z","shell.execute_reply.started":"2023-12-09T16:56:33.334473Z","shell.execute_reply":"2023-12-09T16:56:33.338160Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def scaled_dot_product(query, key, val, mask=None):\n    d_k = query.size()[-1]\n    scaled = torch.matmul(query, key.transpose(-1, -2)) / math.sqrt(d_k)\n    if mask is not None:\n        scaled = scaled.permute(1, 0, 2, 3) + mask\n        scaled = scaled.permute(1, 0, 2, 3)\n    attention = Func.softmax(scaled, dim=-1)\n    values = torch.matmul(attention, value)\n    return values, attention","metadata":{"execution":{"iopub.status.busy":"2023-12-09T16:57:32.899820Z","iopub.execute_input":"2023-12-09T16:57:32.900734Z","iopub.status.idle":"2023-12-09T16:57:32.906817Z","shell.execute_reply.started":"2023-12-09T16:57:32.900687Z","shell.execute_reply":"2023-12-09T16:57:32.905710Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_sequence_length):\n        super().__init__()\n        self.max_sequence_length = max_sequence_length\n        self.d_model = d_model","metadata":{"execution":{"iopub.status.busy":"2023-12-09T16:57:47.081394Z","iopub.execute_input":"2023-12-09T16:57:47.081740Z","iopub.status.idle":"2023-12-09T16:57:47.087386Z","shell.execute_reply.started":"2023-12-09T16:57:47.081711Z","shell.execute_reply":"2023-12-09T16:57:47.086231Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def forward(self):\n    even_i = torch.arange(0, self.d_model, 2).float()\n    denominator = torch.pow(10000, even_i/self.d_model)\n    position = (torch.arange(self.max_sequence_length)\n                      .reshape(self.max_sequence_length, 1))\n    even_PE = torch.sin(position / denominator)\n    odd_PE = torch.cos(position / denominator)\n    stacked = torch.stack([even_PE, odd_PE], dim=2)\n    PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n    return PE","metadata":{"execution":{"iopub.status.busy":"2023-12-09T16:58:46.474202Z","iopub.execute_input":"2023-12-09T16:58:46.474551Z","iopub.status.idle":"2023-12-09T16:58:46.481295Z","shell.execute_reply.started":"2023-12-09T16:58:46.474522Z","shell.execute_reply":"2023-12-09T16:58:46.480126Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class SentenceEmbedding(nn.Module):\n    \"For a given sentence, create an embedding\"\n    def __init__(self, max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN):\n        super().__init__()\n        self.vocab_size = len(language_to_index)\n        self.max_sequence_length = max_sequence_length\n        self.embedding = nn.Embedding(self.vocab_size, d_model)\n        self.language_to_index = language_to_index\n        self.position_encoder = PositionalEncoding(d_model, max_sequence_length)\n        self.dropout = nn.Dropout(p=0.1)\n        self.START_TOKEN = START_TOKEN\n        self.END_TOKEN = END_TOKEN\n        self.PADDING_TOKEN = PADDING_TOKEN\n    \n    def batch_tokenize(self, batch, start_token, end_token):\n\n        def tokenize(sentence, start_token, end_token):\n            sentence_word_indicies = [self.language_to_index[token] for token in list(sentence)]\n            if start_token:\n                sentence_word_indicies.insert(0, self.language_to_index[self.START_TOKEN])\n            if end_token:\n                sentence_word_indicies.append(self.language_to_index[self.END_TOKEN])\n            for _ in range(len(sentence_word_indicies), self.max_sequence_length):\n                sentence_word_indicies.append(self.language_to_index[self.PADDING_TOKEN])\n            return torch.tensor(sentence_word_indicies)\n\n        tokenized = []\n        for sentence_num in range(len(batch)):\n           tokenized.append( tokenize(batch[sentence_num], start_token, end_token) )\n        tokenized = torch.stack(tokenized)\n        return tokenized.to(get_device())\n    \n    def forward(self, x, start_token, end_token): # sentence\n        x = self.batch_tokenize(x, start_token, end_token)\n        x = self.embedding(x)\n        pos = self.position_encoder().to(get_device())\n        x = self.dropout(x + pos)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-12-09T16:59:57.476483Z","iopub.execute_input":"2023-12-09T16:59:57.476844Z","iopub.status.idle":"2023-12-09T16:59:57.489001Z","shell.execute_reply.started":"2023-12-09T16:59:57.476814Z","shell.execute_reply":"2023-12-09T16:59:57.488071Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.head_dim = d_model // num_heads\n        self.qkv_layer = nn.Linear(d_model , 3 * d_model)\n        self.linear_layer = nn.Linear(d_model, d_model)\n    \n    def forward(self, x, mask):\n        batch_size, sequence_length, d_model = x.size()\n        qkv = self.qkv_layer(x)\n        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim)\n        qkv = qkv.permute(0, 2, 1, 3)\n        q, k, v = qkv.chunk(3, dim=-1)\n        values, attention = scaled_dot_product(q, k, v, mask)\n        values = values.permute(0, 2, 1, 3).reshape(batch_size, sequence_length, self.num_heads * self.head_dim)\n        out = self.linear_layer(values)\n        return out\n","metadata":{"execution":{"iopub.status.busy":"2023-12-09T17:00:16.537987Z","iopub.execute_input":"2023-12-09T17:00:16.538353Z","iopub.status.idle":"2023-12-09T17:00:16.546374Z","shell.execute_reply.started":"2023-12-09T17:00:16.538323Z","shell.execute_reply":"2023-12-09T17:00:16.545445Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class LayerNormalization(nn.Module):\n    def __init__(self, parameters_shape, eps=1e-5):\n        super().__init__()\n        self.parameters_shape=parameters_shape\n        self.eps=eps\n        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n        self.beta =  nn.Parameter(torch.zeros(parameters_shape))\n\n    def forward(self, inputs):\n        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n        mean = inputs.mean(dim=dims, keepdim=True)\n        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n        std = (var + self.eps).sqrt()\n        y = (inputs - mean) / std\n        out = self.gamma * y + self.beta\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-12-09T17:00:40.326374Z","iopub.execute_input":"2023-12-09T17:00:40.326706Z","iopub.status.idle":"2023-12-09T17:00:40.336379Z","shell.execute_reply.started":"2023-12-09T17:00:40.326681Z","shell.execute_reply":"2023-12-09T17:00:40.335406Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class PositionwiseFeedForward(nn.Module):\n    def __init__(self, d_model, hidden, drop_prob=0.1):\n        super(PositionwiseFeedForward, self).__init__()\n        self.linear1 = nn.Linear(d_model, hidden)\n        self.linear2 = nn.Linear(hidden, d_model)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=drop_prob)\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.linear2(x)\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2023-12-09T17:00:50.085522Z","iopub.execute_input":"2023-12-09T17:00:50.086157Z","iopub.status.idle":"2023-12-09T17:00:50.092960Z","shell.execute_reply.started":"2023-12-09T17:00:50.086124Z","shell.execute_reply":"2023-12-09T17:00:50.091945Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class EncoderLayer(nn.Module):\n    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n        super(EncoderLayer, self).__init__()\n        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n        self.dropout1 = nn.Dropout(p=drop_prob)\n        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n        self.dropout2 = nn.Dropout(p=drop_prob)\n\n    def forward(self, x, self_attention_mask):\n        residual_x = x.clone()\n        x = self.attention(x, mask=self_attention_mask)\n        x = self.dropout1(x)\n        x = self.norm1(x + residual_x)\n        residual_x = x.clone()\n        x = self.ffn(x)\n        x = self.dropout2(x)\n        x = self.norm2(x + residual_x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-12-09T17:01:01.012620Z","iopub.execute_input":"2023-12-09T17:01:01.013570Z","iopub.status.idle":"2023-12-09T17:01:01.022620Z","shell.execute_reply.started":"2023-12-09T17:01:01.013535Z","shell.execute_reply":"2023-12-09T17:01:01.021725Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class SequentialEncoder(nn.Sequential):\n    def forward(self, *inputs):\n        x, self_attention_mask  = inputs\n        for module in self._modules.values():\n            x = module(x, self_attention_mask)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-12-09T17:01:10.884813Z","iopub.execute_input":"2023-12-09T17:01:10.885650Z","iopub.status.idle":"2023-12-09T17:01:10.891660Z","shell.execute_reply.started":"2023-12-09T17:01:10.885612Z","shell.execute_reply":"2023-12-09T17:01:10.890769Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, \n                 d_model, \n                 ffn_hidden, \n                 num_heads, \n                 drop_prob, \n                 num_layers,\n                 max_sequence_length,\n                 language_to_index,\n                 START_TOKEN,\n                 END_TOKEN, \n                 PADDING_TOKEN):\n        super().__init__()\n        self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n        self.layers = SequentialEncoder(*[EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob)\n                                      for _ in range(num_layers)])\n\n    def forward(self, x, self_attention_mask, start_token, end_token):\n        x = self.sentence_embedding(x, start_token, end_token)\n        x = self.layers(x, self_attention_mask)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-12-09T17:01:18.860524Z","iopub.execute_input":"2023-12-09T17:01:18.860859Z","iopub.status.idle":"2023-12-09T17:01:18.868181Z","shell.execute_reply.started":"2023-12-09T17:01:18.860833Z","shell.execute_reply":"2023-12-09T17:01:18.867116Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class MultiHeadCrossAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.head_dim = d_model // num_heads\n        self.kv_layer = nn.Linear(d_model , 2 * d_model)\n        self.q_layer = nn.Linear(d_model , d_model)\n        self.linear_layer = nn.Linear(d_model, d_model)\n    \n    def forward(self, x, y, mask):\n        batch_size, sequence_length, d_model = x.size() # in practice, this is the same for both languages...so we can technically combine with normal attention\n        kv = self.kv_layer(x)\n        q = self.q_layer(y)\n        kv = kv.reshape(batch_size, sequence_length, self.num_heads, 2 * self.head_dim)\n        q = q.reshape(batch_size, sequence_length, self.num_heads, self.head_dim)\n        kv = kv.permute(0, 2, 1, 3)\n        q = q.permute(0, 2, 1, 3)\n        k, v = kv.chunk(2, dim=-1)\n        values, attention = scaled_dot_product(q, k, v, mask) # We don't need the mask for cross attention, removing in outer function!\n        values = values.permute(0, 2, 1, 3).reshape(batch_size, sequence_length, d_model)\n        out = self.linear_layer(values)\n        return out\n","metadata":{"execution":{"iopub.status.busy":"2023-12-09T17:01:33.122420Z","iopub.execute_input":"2023-12-09T17:01:33.122761Z","iopub.status.idle":"2023-12-09T17:01:33.132493Z","shell.execute_reply.started":"2023-12-09T17:01:33.122724Z","shell.execute_reply":"2023-12-09T17:01:33.131592Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"class DecoderLayer(nn.Module):\n    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n        super(DecoderLayer, self).__init__()\n        self.self_attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n        self.layer_norm1 = LayerNormalization(parameters_shape=[d_model])\n        self.dropout1 = nn.Dropout(p=drop_prob)\n\n        self.encoder_decoder_attention = MultiHeadCrossAttention(d_model=d_model, num_heads=num_heads)\n        self.layer_norm2 = LayerNormalization(parameters_shape=[d_model])\n        self.dropout2 = nn.Dropout(p=drop_prob)\n\n        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n        self.layer_norm3 = LayerNormalization(parameters_shape=[d_model])\n        self.dropout3 = nn.Dropout(p=drop_prob)\n\n    def forward(self, x, y, self_attention_mask, cross_attention_mask):\n        _y = y.clone()\n        y = self.self_attention(y, mask=self_attention_mask)\n        y = self.dropout1(y)\n        y = self.layer_norm1(y + _y)\n\n        _y = y.clone()\n        y = self.encoder_decoder_attention(x, y, mask=cross_attention_mask)\n        y = self.dropout2(y)\n        y = self.layer_norm2(y + _y)\n\n        _y = y.clone()\n        y = self.ffn(y)\n        y = self.dropout3(y)\n        y = self.layer_norm3(y + _y)\n        return y","metadata":{"execution":{"iopub.status.busy":"2023-12-09T17:01:43.780002Z","iopub.execute_input":"2023-12-09T17:01:43.780918Z","iopub.status.idle":"2023-12-09T17:01:43.793195Z","shell.execute_reply.started":"2023-12-09T17:01:43.780876Z","shell.execute_reply":"2023-12-09T17:01:43.792102Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class SequentialDecoder(nn.Sequential):\n    def forward(self, *inputs):\n        x, y, self_attention_mask, cross_attention_mask = inputs\n        for module in self._modules.values():\n            y = module(x, y, self_attention_mask, cross_attention_mask)\n        return y","metadata":{"execution":{"iopub.status.busy":"2023-12-09T17:01:51.543373Z","iopub.execute_input":"2023-12-09T17:01:51.544064Z","iopub.status.idle":"2023-12-09T17:01:51.548948Z","shell.execute_reply.started":"2023-12-09T17:01:51.544029Z","shell.execute_reply":"2023-12-09T17:01:51.548007Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, \n                 d_model, \n                 ffn_hidden, \n                 num_heads, \n                 drop_prob, \n                 num_layers,\n                 max_sequence_length,\n                 language_to_index,\n                 START_TOKEN,\n                 END_TOKEN, \n                 PADDING_TOKEN):\n        super().__init__()\n        self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n        self.layers = SequentialDecoder(*[DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)])\n\n    def forward(self, x, y, self_attention_mask, cross_attention_mask, start_token, end_token):\n        y = self.sentence_embedding(y, start_token, end_token)\n        y = self.layers(x, y, self_attention_mask, cross_attention_mask)\n        return y","metadata":{"execution":{"iopub.status.busy":"2023-12-09T17:01:58.873816Z","iopub.execute_input":"2023-12-09T17:01:58.874221Z","iopub.status.idle":"2023-12-09T17:01:58.881415Z","shell.execute_reply.started":"2023-12-09T17:01:58.874189Z","shell.execute_reply":"2023-12-09T17:01:58.880401Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self, \n                d_model, \n                ffn_hidden, \n                num_heads, \n                drop_prob, \n                num_layers,\n                max_sequence_length, \n                kn_vocab_size,\n                english_to_index,\n                kannada_to_index,\n                START_TOKEN, \n                END_TOKEN, \n                PADDING_TOKEN\n                ):\n        super().__init__()\n        self.encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, english_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n        self.decoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, kannada_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n        self.linear = nn.Linear(d_model, kn_vocab_size)\n        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n    def forward(self, \n                x, \n                y, \n                encoder_self_attention_mask=None, \n                decoder_self_attention_mask=None, \n                decoder_cross_attention_mask=None,\n                enc_start_token=False,\n                enc_end_token=False,\n                dec_start_token=False, # We should make this true\n                dec_end_token=False): # x, y are batch of sentences\n        x = self.encoder(x, encoder_self_attention_mask, start_token=enc_start_token, end_token=enc_end_token)\n        out = self.decoder(x, y, decoder_self_attention_mask, decoder_cross_attention_mask, start_token=dec_start_token, end_token=dec_end_token)\n        out = self.linear(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-12-09T17:02:10.011981Z","iopub.execute_input":"2023-12-09T17:02:10.012898Z","iopub.status.idle":"2023-12-09T17:02:10.022305Z","shell.execute_reply.started":"2023-12-09T17:02:10.012863Z","shell.execute_reply":"2023-12-09T17:02:10.021239Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}